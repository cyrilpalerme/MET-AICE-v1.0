{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a0954034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import netCDF4\n",
    "import scipy\n",
    "import pyproj\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56ad58",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49406ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 15\n",
    "#\n",
    "date_min = \"20220101\"\n",
    "date_max = \"20250331\"\n",
    "#\n",
    "paths = {}\n",
    "paths[\"AICE_op_forecasts\"] = \"/lustre/storeB/project/fou/hi/oper/aice/archive/\"\n",
    "paths[\"AICE_reforecasts\"] = \"/lustre/storeB/project/copernicus/cosi/AICE/Predictions/AICE_v1_reforecasts/\"\n",
    "paths[\"IFS\"] = \"/lustre/storeB/project/copernicus/cosi/AICE/Data/ECMWF_daily_time_steps/\"\n",
    "paths[\"IFS_hourly\"] = \"/lustre/storeB/project/copernicus/cosi/AICE/Data/ECMWF_first_hourly_time_step/\"\n",
    "paths[\"TOPAZ5\"] = \"/lustre/storeB/project/copernicus/sea/metnotopaz5/arctic/mersea-class1/\"\n",
    "paths[\"TOPAZ5_hourly\"] = \"/lustre/storeB/project/copernicus/sea/metnotopaz5/arctic_1hr/mersea-class1/\"\n",
    "paths[\"Barents\"] = \"/lustre/storeB/project/fou/hi/oper/barents_eps/archive/surface/\"\n",
    "paths[\"output\"] = \"/lustre/storeB/project/copernicus/cosi/AICE/Data/Models_on_AICE_grid/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467f544",
   "metadata": {},
   "source": [
    "# List dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c810559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max):\n",
    "    current_date = datetime.datetime.strptime(date_min, \"%Y%m%d\")\n",
    "    end_date = datetime.datetime.strptime(date_max, \"%Y%m%d\")\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        list_dates.append(date_str)\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    return list_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d7af0",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b611506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_datasets():\n",
    "    def __init__(self, date_task, paths):\n",
    "        self.date_task = date_task\n",
    "        self.paths = paths\n",
    "\n",
    "    def read_AICE(self):\n",
    "        Dataset = {}\n",
    "        if datetime.datetime.strptime(self.date_task, \"%Y%m%d\") >= datetime.datetime.strptime(\"20240401\", \"%Y%m%d\"):\n",
    "            filename = self.paths[\"AICE_op_forecasts\"] + \"AICE_forecasts_\" + self.date_task + \"T000000Z.nc\"\n",
    "        else:\n",
    "            filename = self.paths[\"AICE_reforecasts\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + \"AICE_forecasts_\" + self.date_task + \"T000000Z.nc\"\n",
    "        if os.path.isfile(filename) == True:\n",
    "            with netCDF4.Dataset(filename, \"r\") as nc:\n",
    "                for var in nc.variables:\n",
    "                    if var == \"Lambert_Azimuthal_Grid\":\n",
    "                        Dataset[\"proj4\"] = nc.variables[var].proj4_string\n",
    "                    else:\n",
    "                        Dataset[var] = nc.variables[var][:]\n",
    "            Dataset[\"sea_mask\"] = np.ones(np.shape(Dataset[\"lat\"]))\n",
    "            Dataset[\"sea_mask\"][np.isnan(Dataset[\"SIC\"][0,:,:]) == True] = 0\n",
    "        return Dataset\n",
    "    \n",
    "    def read_IFS(self):\n",
    "        Dataset = {}\n",
    "        filename = self.paths[\"IFS\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + \"ECMWF_operational_forecasts_daily_time_steps_SIC_\" + self.date_task + \".nc\"\n",
    "        if os.path.isfile(filename) == True:\n",
    "            with netCDF4.Dataset(filename, \"r\") as nc:\n",
    "                for var in nc.variables:\n",
    "                    if var == \"CI\":\n",
    "                        Dataset[\"SIC\"] = nc.variables[var][:] * 100\n",
    "                    else:\n",
    "                        Dataset[var] = nc.variables[var][:]\n",
    "            Dataset[\"proj4\"] = \"+proj=latlon\"\n",
    "\n",
    "            filename_hourly = self.paths[\"IFS_hourly\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + \"ECMWF_operational_forecasts_SIC_\" + self.date_task + \"_NH_only_t00.nc\"\n",
    "            with netCDF4.Dataset(filename_hourly, \"r\") as nc_hourly:\n",
    "                Dataset[\"SIC_t0\"] = nc_hourly.variables[\"CI\"][0,:,:] * 100\n",
    "            \n",
    "            filename_land_sea_mask = self.paths[\"IFS\"] + \"ECMWF_operational_forecasts_Land_Sea_Mask.nc\"\n",
    "            with netCDF4.Dataset(filename_land_sea_mask, \"r\") as nc:\n",
    "                LSM = np.squeeze(nc.variables[\"LSM\"][:])\n",
    "                Dataset[\"sea_mask\"] = np.zeros(np.shape(LSM))\n",
    "                Dataset[\"sea_mask\"][LSM == 0] = 1\n",
    "\n",
    "        return Dataset\n",
    "\n",
    "    def read_TOPAZ5(self):\n",
    "        Dataset = {}\n",
    "        max_lead_time = 10\n",
    "        if datetime.datetime.strptime(self.date_task, \"%Y%m%d\") >= datetime.datetime.strptime(\"20230901\", \"%Y%m%d\"):\n",
    "            for lt in range(0, max_lead_time):\n",
    "                forecast_date = (datetime.datetime.strptime(self.date_task, \"%Y%m%d\") + datetime.timedelta(days = lt)).strftime(\"%Y%m%d\")\n",
    "                filename = self.paths[\"TOPAZ5\"] + forecast_date[0:4] + \"/\" + forecast_date[4:6] + \"/\" + forecast_date + \"_dm-metno-MODEL-topaz5-ARC-b\" + self.date_task + \"-fv02.0.nc\"\n",
    "                if os.path.isfile(filename) == True:\n",
    "                    with netCDF4.Dataset(filename, \"r\") as nc:\n",
    "                        if lt == 0:\n",
    "                            Dataset[\"proj4\"] = nc.variables[\"stereographic\"].proj4\n",
    "                            Dataset[\"x\"] = nc.variables[\"x\"][:] * 100 * 1000\n",
    "                            Dataset[\"y\"] = nc.variables[\"y\"][:] * 100 * 1000\n",
    "                            Dataset[\"lat\"] = nc.variables[\"latitude\"][:,:]\n",
    "                            Dataset[\"lon\"] = nc.variables[\"longitude\"][:,:]\n",
    "                            Dataset[\"SIC\"] = nc.variables[\"siconc\"][:,:,:] * 100\n",
    "                            Dataset[\"sea_mask\"] = np.ones(np.shape(Dataset[\"lat\"]))\n",
    "                            Dataset[\"sea_mask\"][np.squeeze(Dataset[\"SIC\"].mask) == True] = 0\n",
    "                            Dataset[\"SIC\"][np.expand_dims(Dataset[\"sea_mask\"] == 0, axis = 0)] = np.nan\n",
    "\n",
    "                            filename_hourly = self.paths[\"TOPAZ5_hourly\"] + forecast_date[0:4] + \"/\" + forecast_date[4:6] + \"/\" + forecast_date + \"_hr-metno-MODEL-topaz5-ARC-b\" + self.date_task + \"-fv02.0.nc\"\n",
    "                            if os.path.isfile(filename_hourly) == True:\n",
    "                                with netCDF4.Dataset(filename_hourly, \"r\") as nc_hourly:\n",
    "                                    Dataset[\"SIC_t0\"] = nc_hourly[\"siconc\"][0,:,:] * 100\n",
    "                        else:\n",
    "                            SIC = nc.variables[\"siconc\"][:,:,:] * 100\n",
    "                            SIC[np.expand_dims(Dataset[\"sea_mask\"] == 0, axis = 0)] = np.nan\n",
    "                            Dataset[\"SIC\"] = np.concatenate((Dataset[\"SIC\"], SIC), axis = 0)\n",
    "                else:\n",
    "                    if lt > 0:\n",
    "                        SIC_nan = np.expand_dims(np.full(np.shape(Dataset[\"lat\"]), np.nan), axis = 0)\n",
    "                        Dataset[\"SIC\"] = np.concatenate((Dataset[\"SIC\"], SIC_nan), axis = 0)\n",
    "                    else:\n",
    "                        return Dataset\n",
    "        return Dataset\n",
    "                \n",
    "    def read_Barents(self):\n",
    "        Dataset = {}\n",
    "        prod_time = \"T00Z\"\n",
    "        N_Barents_members = 6\n",
    "\n",
    "        if datetime.datetime.strptime(self.date_task, \"%Y%m%d\") >= datetime.datetime.strptime(\"20231206\", \"%Y%m%d\"):\n",
    "            path_data = self.paths[\"Barents\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\" + self.date_task[6:8] + \"/\" + prod_time + \"/\"\n",
    "            for em in range(0, N_Barents_members):\n",
    "                member = \"{:02d}\".format(em)\n",
    "                filename = path_data + \"barents_sfc_\" + self.date_task + prod_time + \"m\" + member + \".nc\"\n",
    "                if os.path.isfile(filename) == True:\n",
    "                    with netCDF4.Dataset(filename, \"r\") as nc:\n",
    "                        if member == \"00\":\n",
    "                            Dataset[\"proj4\"] = nc.variables[\"projection_lambert\"].proj4\n",
    "                            Dataset[\"x\"] = nc.variables[\"X\"][:]    \n",
    "                            Dataset[\"y\"] = nc.variables[\"Y\"][:]  \n",
    "                            Dataset[\"sea_mask\"] = nc.variables[\"sea_mask\"][:]\n",
    "\n",
    "                        Dataset[\"Member\" + member + \"_SIC_t0\"] = nc.variables[\"ice_concentration\"][0,:,:] * 100\n",
    "\n",
    "                        Dataset[\"Member\" + member + \"_SIC\"] = np.full((4, len(Dataset[\"y\"]), len(Dataset[\"x\"])), np.nan) \n",
    "                        for ts in range(0, 4):\n",
    "                            ts_start = ts * 24\n",
    "                            ts_end = ts_start + 24\n",
    "                            Dataset[\"Member\" + member + \"_SIC\"][ts,:,:] = np.nanmean(nc.variables[\"ice_concentration\"][ts_start:ts_end,:,:], axis = 0) * 100\n",
    "        return Dataset\n",
    "                    \n",
    "    def __call__(self):\n",
    "        All_datasets = {}\n",
    "        All_datasets[\"AICE\"] = self.read_AICE()\n",
    "        All_datasets[\"IFS\"] = self.read_IFS()\n",
    "        All_datasets[\"TOPAZ5\"] = self.read_TOPAZ5()\n",
    "        All_datasets[\"Barents\"] = self.read_Barents()\n",
    "        return All_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee02d50",
   "metadata": {},
   "source": [
    " # Regridding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "987a2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class regridding():\n",
    "    def __init__(self, date_task, Model_data):\n",
    "        self.date_task = date_task\n",
    "        self.Model_data = Model_data\n",
    "    \n",
    "    def make_padding(self, x, y, field):\n",
    "        dx = x[1] - x[0]\n",
    "        x_extent = np.pad(x, (1, 1), constant_values = np.nan)    \n",
    "        x_extent[0] = x_extent[1] - dx\n",
    "        x_extent[-1] = x_extent[-2] + dx\n",
    "        \n",
    "        dy = y[1] - y[0]\n",
    "        y_extent = np.pad(y, (1, 1), constant_values = np.nan)\n",
    "        y_extent[0] = y_extent[1] - dy\n",
    "        y_extent[-1] = y_extent[-2] + dy\n",
    "        \n",
    "        if field.ndim == 2:\n",
    "            field_extent = np.pad(field, (1,1), constant_values = np.nan)\n",
    "        elif field.ndim == 3:\n",
    "            time_dim = len(field[:,0,0])\n",
    "            field_extent = np.full((time_dim, len(y_extent), len(x_extent)), np.nan)\n",
    "            \n",
    "            for t in range(0, time_dim):\n",
    "                field_extent[t,:,:] = np.pad(field[t,:,:], (1,1), constant_values = np.nan)\n",
    "        \n",
    "        return x_extent, y_extent, field_extent\n",
    "    \n",
    "    def nearest_neighbor_indexes(self, x_input, y_input, x_output, y_output):\n",
    "        x_input = np.expand_dims(x_input, axis = 1)\n",
    "        y_input = np.expand_dims(y_input, axis = 1)\n",
    "        x_output = np.expand_dims(x_output, axis = 1)\n",
    "        y_output = np.expand_dims(y_output, axis = 1)\n",
    "        \n",
    "        coord_input = np.concatenate((x_input, y_input), axis = 1)\n",
    "        coord_output = np.concatenate((x_output, y_output), axis = 1)\n",
    "        \n",
    "        tree = scipy.spatial.KDTree(coord_input)\n",
    "        dist, idx = tree.query(coord_output)\n",
    "        return idx\n",
    "    \n",
    "    def extract_idx(self, lat, lon):\n",
    "        transform = pyproj.Transformer.from_crs(pyproj.CRS.from_proj4(\"+proj=latlon\"), pyproj.CRS.from_proj4(self.Model_data[\"AICE\"][\"proj4\"]), always_xy = True)\n",
    "        xx_input, yy_input = transform.transform(lon, lat)   \n",
    "        \n",
    "        xx_input = np.ndarray.flatten(xx_input)\n",
    "        yy_input = np.ndarray.flatten(yy_input)\n",
    "        \n",
    "        xx_output, yy_output = np.meshgrid(self.Model_data[\"AICE\"][\"x\"], self.Model_data[\"AICE\"][\"y\"])\n",
    "        xx_output = np.ndarray.flatten(xx_output)\n",
    "        yy_output = np.ndarray.flatten(yy_output)\n",
    "        \n",
    "        idx = self.nearest_neighbor_indexes(xx_input, yy_input, xx_output, yy_output)\n",
    "        return idx\n",
    "    \n",
    "    def nearest_neighbor_interpolation(self):\n",
    "        Interpolated_datasets = {}\n",
    "        \n",
    "        for model in self.Model_data:\n",
    "            Interpolated_datasets[model] = {}\n",
    "            \n",
    "            if model == \"AICE\":\n",
    "                for var in self.Model_data[model]:\n",
    "                    Interpolated_datasets[model][var] = np.copy(self.Model_data[model][var])\n",
    "            else:\n",
    "                if len(self.Model_data[model]) > 0:                   \n",
    "                    for var in self.Model_data[model]:\n",
    "                        if (var == \"sea_mask\") or (\"SIC\" in var):\n",
    "                            if \"x\" in self.Model_data[model]:\n",
    "                                x_pad, y_pad, var_pad = self.make_padding(self.Model_data[model][\"x\"], self.Model_data[model][\"y\"], self.Model_data[model][var])\n",
    "                                xx_pad, yy_pad = np.meshgrid(x_pad, y_pad)\n",
    "                                transform = pyproj.Transformer.from_crs(pyproj.CRS.from_proj4(self.Model_data[model][\"proj4\"]), pyproj.CRS.from_proj4(\"+proj=latlon\"), always_xy = True)\n",
    "                                lon_pad, lat_pad = transform.transform(xx_pad, yy_pad)\n",
    "                            else:\n",
    "                                if np.max(self.Model_data[model][\"lat\"]) == 90:\n",
    "                                    self.Model_data[model][\"lat\"][self.Model_data[model][\"lat\"] == 90] = 89.999999999  # In order to avoid interpolating nan (see 2 lines below)\n",
    "                                lon_1D_pad, lat_1D_pad, var_pad = self.make_padding(self.Model_data[model][\"lon\"], self.Model_data[model][\"lat\"], self.Model_data[model][var])\n",
    "                                lat_1D_pad[lat_1D_pad > 90] = 90  # In order to avoid latitudes > 90 after padding\n",
    "                                lon_pad, lat_pad = np.meshgrid(lon_1D_pad, lat_1D_pad)\n",
    "                            idx = self.extract_idx(lat_pad, lon_pad)\n",
    "\n",
    "                            if (var == \"sea_mask\") or (\"SIC_t0\" in var):\n",
    "                                field_flat = np.ndarray.flatten(var_pad)\n",
    "                                field_interp = field_flat[idx]\n",
    "                                field_regrid = np.reshape(field_interp, (len(self.Model_data[\"AICE\"][\"y\"]), len(self.Model_data[\"AICE\"][\"x\"])), order = \"C\")\n",
    "                            elif (\"SIC\" in var) and (\"SIC_t0\" not in var):\n",
    "                                time_dim = len(self.Model_data[model][var][:,0,0])\n",
    "                                field_regrid = np.full((time_dim, len(self.Model_data[\"AICE\"][\"y\"]), len(self.Model_data[\"AICE\"][\"x\"])), np.nan)\n",
    "                                for t in range(0, time_dim):\n",
    "                                    field_flat = np.ndarray.flatten(var_pad[t,:,:])\n",
    "                                    field_interp = field_flat[idx]\n",
    "                                    field_regrid[t,:,:] = np.reshape(field_interp, (len(self.Model_data[\"AICE\"][\"y\"]), len(self.Model_data[\"AICE\"][\"x\"])), order = \"C\")\n",
    "                            \n",
    "                            Interpolated_datasets[model][var] = np.copy(field_regrid)\n",
    "        return Interpolated_datasets\n",
    "    #\n",
    "    def __call__(self):\n",
    "        Interpolated_datasets = self.nearest_neighbor_interpolation()\n",
    "        return Interpolated_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb07ab3",
   "metadata": {},
   "source": [
    "# Write netCDF output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e2d566b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class write_netCDF():\n",
    "    def __init__(self, Interpolated_datasets, date_task, paths):\n",
    "        self.Interpolated_datasets = Interpolated_datasets\n",
    "        self.date_task = date_task\n",
    "        self.paths = paths\n",
    "    \n",
    "    def make_common_sea_mask(self):\n",
    "        Common_sea_mask = np.ones(np.shape(self.Interpolated_datasets[\"AICE\"][\"sea_mask\"]))\n",
    "        Common_domain_mask = np.ones(np.shape(self.Interpolated_datasets[\"AICE\"][\"sea_mask\"]))\n",
    "        \n",
    "        for model in self.Interpolated_datasets:\n",
    "            if len(self.Interpolated_datasets[model]) > 0:\n",
    "                sea_mask = self.Interpolated_datasets[model][\"sea_mask\"]\n",
    "                Common_sea_mask[sea_mask < 1] = 0\n",
    "                \n",
    "                if \"SIC\" in self.Interpolated_datasets[model]:\n",
    "                    SIC_t0 = self.Interpolated_datasets[model][\"SIC\"][0,:,:]\n",
    "                    Common_domain_mask[np.isnan(SIC_t0) == True] = 0\n",
    "                if \"Member00_SIC\" in self.Interpolated_datasets[model]:\n",
    "                    SIC_t0 = self.Interpolated_datasets[model][\"Member00_SIC\"][0,:,:]\n",
    "                    Common_domain_mask[np.isnan(SIC_t0) == True] = 0    \n",
    "        \n",
    "        Common_domain_mask[Common_sea_mask == 0] = 0\n",
    "        return Common_sea_mask, Common_domain_mask\n",
    "    \n",
    "    def expand_grid(self, model, Sea_mask, var):\n",
    "        time_dim_model = np.shape(self.Interpolated_datasets[model][var])[0]\n",
    "        time_dim = len(self.Interpolated_datasets[\"AICE\"][\"time\"])\n",
    "        y_dim = len(self.Interpolated_datasets[\"AICE\"][\"y\"])\n",
    "        x_dim = len(self.Interpolated_datasets[\"AICE\"][\"x\"])\n",
    "        Sea_mask_extend = np.repeat(np.expand_dims(Sea_mask, axis = 0), time_dim, axis = 0)\n",
    "        SIC = np.full((time_dim, y_dim, x_dim), np.nan)\n",
    "        SIC[0:time_dim_model,:,:] = self.Interpolated_datasets[model][var]\n",
    "        SIC[Sea_mask_extend < 1] = np.nan\n",
    "        return SIC\n",
    "\n",
    "    def write_output_file(self, Sea_mask, Domain_mask):\n",
    "        path_output_date = self.paths[\"output\"] + self.date_task[0:4] + \"/\" + self.date_task[4:6] + \"/\"\n",
    "        if os.path.exists(path_output_date) == False:\n",
    "            os.system(\"mkdir -p \" + path_output_date)\n",
    "        output_filename = path_output_date + \"Models_SIC_\" + self.date_task + \"_on_AICE_grid.nc\"\n",
    "        if os.path.isfile(output_filename):\n",
    "            os.system(\"rm \" + output_filename)\n",
    "        \n",
    "        with netCDF4.Dataset(str(output_filename), \"w\", format = \"NETCDF4\") as output_netcdf:\n",
    "            Lambert_Azimuthal_Grid = output_netcdf.createDimension(\"proj4\", 1)\n",
    "            time = output_netcdf.createDimension(\"time\", len(self.Interpolated_datasets[\"AICE\"][\"time\"]))\n",
    "            x = output_netcdf.createDimension(\"x\", len(self.Interpolated_datasets[\"AICE\"][\"x\"]))\n",
    "            y = output_netcdf.createDimension(\"y\", len(self.Interpolated_datasets[\"AICE\"][\"y\"]))\n",
    "            \n",
    "            Outputs = vars()\n",
    "            \n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"] = output_netcdf.createVariable(\"Lambert_Azimuthal_Grid\", \"d\")\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].grid_mapping_name = \"lambert_azimuthal_equal_area\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].semi_major_axis = 6378137\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].semi_minor_axis = 6356752.31424518\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].reference_ellipsoid_name = \"WGS 84\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].longitude_of_prime_meridian = \"0.0\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].prime_meridian_name = \"Greenwich\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].geographic_crs_name = \"unknown\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].horizontal_datum_name = \"Unknown based on WGS84 ellipsoid\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].projected_crs_name = \"unknown\"\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].latitude_of_projection_origin = 90.0\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].longitude_of_projection_origin = 0.0\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].false_easting = 0.0\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].false_northing = 0.0\n",
    "            Outputs[\"Lambert_Azimuthal_Grid\"].proj4_string = \"+ellps=WGS84 +lat_0=90 +lon_0=0 +no_defs=None +proj=laea +type=crs +units=m +x_0=0 +y_0=0\"\n",
    "            \n",
    "            Outputs[\"time\"] = output_netcdf.createVariable(\"time\", \"d\", (\"time\"))\n",
    "            Outputs[\"time\"].units = \"seconds since 1970-01-01 00:00:00 +0000\"\n",
    "            Outputs[\"time\"].standard_name = \"time\"\n",
    "            Outputs[\"time\"].long_name = \"time\"\n",
    "            Outputs[\"time\"][:] = np.copy(self.Interpolated_datasets[\"AICE\"][\"time\"])\n",
    "            \n",
    "            Outputs[\"x\"] = output_netcdf.createVariable(\"x\", \"d\", (\"x\"))\n",
    "            Outputs[\"x\"].units = \"m\"\n",
    "            Outputs[\"x\"].standard_name = \"projection_x_coordinate\"\n",
    "            Outputs[\"x\"].long_name = \"projection_x_coordinate\"\n",
    "            Outputs[\"x\"][:] = np.copy(self.Interpolated_datasets[\"AICE\"][\"x\"])\n",
    "            \n",
    "            Outputs[\"y\"] = output_netcdf.createVariable(\"y\", \"d\", (\"y\"))\n",
    "            Outputs[\"y\"].units = \"m\"\n",
    "            Outputs[\"y\"].standard_name = \"projection_y_coordinate\"\n",
    "            Outputs[\"y\"].long_name = \"projection_y_coordinate\"\n",
    "            Outputs[\"y\"][:] = np.copy(self.Interpolated_datasets[\"AICE\"][\"y\"])\n",
    "            \n",
    "            Outputs[\"sea_mask\"] = output_netcdf.createVariable(\"sea_mask\", \"d\", (\"y\", \"x\"))\n",
    "            Outputs[\"sea_mask\"].units = \"fraction of sea\"\n",
    "            Outputs[\"sea_mask\"].standard_name = \"sea mask\"\n",
    "            Outputs[\"sea_mask\"].long_name = \"sea land mask (0: land, 1: sea)\"\n",
    "            Outputs[\"sea_mask\"][:,:] = np.copy(Sea_mask)\n",
    "            \n",
    "            Outputs[\"domain_mask\"] = output_netcdf.createVariable(\"domain_mask\", \"d\", (\"y\", \"x\"))\n",
    "            Outputs[\"domain_mask\"].units = \"fraction of sea\"\n",
    "            Outputs[\"domain_mask\"].standard_name = \"domain mask\"\n",
    "            Outputs[\"domain_mask\"].long_name = \"Shared domain between all models (0: outside of the shared domain, 1: shared domain)\"\n",
    "            Outputs[\"domain_mask\"][:,:] = np.copy(Domain_mask)\n",
    "            \n",
    "            for model in self.Interpolated_datasets:\n",
    "                for var in self.Interpolated_datasets[model]:\n",
    "                    if \"SIC_t0\" in var: \n",
    "                        print(model, var)\n",
    "                        var_output = model + \"_\" + var\n",
    "                        member = var.replace(\"SIC\", \"\").replace(\"_\", \"\")\n",
    "                        Outputs[var_output] = output_netcdf.createVariable(var_output, \"d\", (\"y\", \"x\"))\n",
    "                        Outputs[var_output].units = \"%\"\n",
    "                        Outputs[var_output].standard_name = model + \" \" + member + \" sea_ice_area_fraction\"\n",
    "                        Outputs[var_output].long_name = model + \" \" + member + \" sea ice concentration\"\n",
    "                        SIC_t0 = self.Interpolated_datasets[model][var]\n",
    "                        print(np.nanmax(SIC_t0))\n",
    "                        SIC_t0[Sea_mask < 1] = np.nan\n",
    "                        Outputs[var_output][:,:] = np.copy(SIC_t0)\n",
    "                    elif (\"SIC\" in var) and (\"SIC_t0\" not in var): \n",
    "                        var_output = model + \"_\" + var\n",
    "                        member = var.replace(\"SIC\", \"\").replace(\"_\", \"\")\n",
    "                        Outputs[var_output] = output_netcdf.createVariable(var_output, \"d\", (\"time\", \"y\", \"x\"))\n",
    "                        Outputs[var_output].units = \"%\"\n",
    "                        Outputs[var_output].standard_name = model + \" \" + member + \" sea_ice_area_fraction\"\n",
    "                        Outputs[var_output].long_name = model + \" \" + member + \" sea ice concentration\"\n",
    "                        Outputs[var_output][:,:,:] = self.expand_grid(model, Sea_mask, var)\n",
    "    \n",
    "    def __call__(self):\n",
    "        Sea_mask, Domain_mask = self.make_common_sea_mask()\n",
    "        self.write_output_file(Sea_mask, Domain_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000d2bc",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ecb58ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFS SIC_t0\n",
      "100.0\n",
      "TOPAZ5 SIC_t0\n",
      "99.99789\n",
      "Barents Member00_SIC_t0\n",
      "1e+30\n",
      "Barents Member01_SIC_t0\n",
      "1e+30\n",
      "Barents Member02_SIC_t0\n",
      "1e+30\n",
      "Barents Member03_SIC_t0\n",
      "1e+30\n",
      "Barents Member04_SIC_t0\n",
      "1e+30\n",
      "Barents Member05_SIC_t0\n",
      "1e+30\n",
      "Computing time:  38.47096920013428\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "list_dates = make_list_dates(date_min = date_min, date_max = date_max)\n",
    "date_task = list_dates[SGE_TASK_ID - 1]\n",
    "\n",
    "Datasets = read_datasets(date_task = date_task, paths = paths)()\n",
    "Interpolated_datasets = regridding(date_task = date_task, Model_data = Datasets)()\n",
    "write_netCDF(Interpolated_datasets = Interpolated_datasets, date_task = date_task, paths = paths)()\n",
    "\n",
    "tf = time.time()\n",
    "print(\"Computing time: \", tf - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "production-08-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
