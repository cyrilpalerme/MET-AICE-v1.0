{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41950ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import scipy\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import datetime \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f1380",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be7d5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGE_TASK_ID = 1\n",
    "#\n",
    "date_min = \"20220326\"\n",
    "date_max = \"20220326\"\n",
    "#\n",
    "lead_time_max = 10\n",
    "#\n",
    "paths = {}\n",
    "paths[\"output\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Training/\"\n",
    "paths[\"ECMWF\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Data/ECMWF/\"\n",
    "paths[\"AMSR2\"] = \"/lustre/storeB/project/copernicus/cosi/WP2/SIC/v0.1/\"\n",
    "#\n",
    "proj = {}\n",
    "proj[\"ECMWF\"] = \"+proj=latlon\"\n",
    "proj[\"AMSR2\"] = \"+ellps=WGS84 +lat_0=90 +lon_0=0 +no_defs=None +proj=laea +type=crs +units=m +x_0=0 +y_0=0\"\n",
    "#\n",
    "crs = {}\n",
    "for var in proj:\n",
    "    crs[var] = pyproj.CRS.from_proj4(proj[var])\n",
    "#\n",
    "variables = {}\n",
    "variables[\"LSM\"] = [\"LSM\"]\n",
    "variables[\"ECMWF\"] = [\"U10M\", \"V10M\", \"T2M\"]\n",
    "variables[\"AMSR2\"] = [\"ice_conc\", \"total_standard_uncertainty\"] \n",
    "#\n",
    "Dates_AMSR2_missing_data = [\"20151204\", \"20160415\", \"20170928\", \"20171125\", \"20181216\", \"20210203\", \"20210620\", \"20210816\", \"20211102\", \"20220324\", \"20220413\", \"20220418\", \"20220729\", \"20221122\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3567e",
   "metadata": {},
   "source": [
    "task_date function\n",
    "\n",
    "    date_min: earliest forecast start date to process\n",
    "    date_max: latest forecast start date to process\n",
    "    task_ID: task ID when parallelizing (SGE_TASK_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97132740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_date(date_min, date_max, task_ID):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_date = []\n",
    "    while current_date <= end_date:\n",
    "        list_date.append(current_date.strftime('%Y%m%d'))\n",
    "        current_date = current_date + datetime.timedelta(days = 1)\n",
    "    date_task = list_date[task_ID - 1]\n",
    "    return(date_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "505ada1d-447c-4d52-8667-de3e564e2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_and_LSM(paths):\n",
    "    xmin = 909\n",
    "    xmax = 1453\n",
    "    ymin = 1075\n",
    "    ymax = 1555\n",
    "    #\n",
    "    Domain_data = {}\n",
    "    #\n",
    "    file_AMSR2 = \"/lustre/storeB/project/copernicus/cosi/WP2/landmasks/LandOceanLakeMask_cosi-ease2-050.nc\"\n",
    "    nc = netCDF4.Dataset(file_AMSR2, \"r\")\n",
    "    Domain_data[\"x\"] = nc.variables[\"xc\"][xmin:xmax] * 1000\n",
    "    Domain_data[\"y\"] = nc.variables[\"yc\"][ymin:ymax] * 1000\n",
    "    Domain_data[\"lat\"] = nc.variables[\"lat\"][ymin:ymax, xmin:xmax]\n",
    "    Domain_data[\"lon\"] = nc.variables[\"lon\"][ymin:ymax, xmin:xmax]\n",
    "    smask = nc.variables[\"smask\"][ymin:ymax, xmin:xmax]\n",
    "    nc.close()\n",
    "    #\n",
    "    LSM = np.zeros(np.shape(smask))\n",
    "    LSM[smask == 0] = 1\n",
    "    Domain_data[\"LSM\"] = np.copy(LSM)\n",
    "    #\n",
    "    print(np.shape(LSM))\n",
    "    #\n",
    "    return(Domain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9165a-610e-4008-b53d-dd79cff52971",
   "metadata": {},
   "source": [
    "# ECMWF_time_steps_to_daily_time_steps function => Compute the daily mean of the variable for each days\n",
    "\n",
    "    time_ECMWF: time variable in ECMWF netCDF files\n",
    "    field: 2D array variable\n",
    "    ndays: number of days (lead time) to compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a1c691c-6de7-4f85-87f4-4c0a082ff50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ECMWF_time_steps_to_daily_time_steps(time_ECMWF, field, ndays):\n",
    "    lead_time = time_ECMWF - time_ECMWF[0]\n",
    "    ts_start = np.linspace(0 * 24, (ndays - 1) * 24, ndays)\n",
    "    ts_end = ts_start + 24\n",
    "    daily_field = np.full((ndays, field.shape[1], field.shape[2]), np.nan)\n",
    "    #\n",
    "    for ts in range(0, ndays):\n",
    "        lead_time_idx = np.squeeze(np.where(np.logical_and(lead_time >= ts_start[ts], lead_time < ts_end[ts])))\n",
    "        if ts == 3:\n",
    "            daily_field[ts,:,:] = (18 * np.nanmean(np.ma.squeeze(field[lead_time_idx[0:18],:,:]), axis = 0) + 6 * np.nanmean(np.ma.squeeze(field[lead_time_idx[18:20],:,:]), axis = 0)) / 24\n",
    "        else:\n",
    "            daily_field[ts,:,:] = np.nanmean(np.ma.squeeze(field[lead_time_idx,:,:]), axis = 0)\n",
    "    #\n",
    "    return(daily_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afc193-808d-4740-98c4-7b4a1ea3028d",
   "metadata": {},
   "source": [
    "# Padding function (make_padding)\n",
    "\n",
    "    x and y must be vectors (can be latitude / longitude if the data are on a regular grid)  \n",
    "    field must be either a 2D array (y, x) or a 3D array (time, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1e719ef-3c8c-4deb-8195-73a747698527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding(x, y, field):\n",
    "    dx = x[1] - x[0]\n",
    "    x_extent = np.pad(x, (1, 1), constant_values = np.nan)    \n",
    "    x_extent[0] = x_extent[1] - dx\n",
    "    x_extent[-1] = x_extent[-2] + dx\n",
    "    #\n",
    "    dy = y[1] - y[0]\n",
    "    y_extent = np.pad(y, (1, 1), constant_values = np.nan)\n",
    "    y_extent[0] = y_extent[1] - dy\n",
    "    y_extent[-1] = y_extent[-2] + dy\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_extent = np.pad(field, (1,1), constant_values = np.nan)\n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_extent = np.full((time_dim, len(y_extent), len(x_extent)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_extent[t,:,:] = np.pad(field[t,:,:], (1,1), constant_values = np.nan)\n",
    "    #\n",
    "    return(x_extent, y_extent, field_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2a134-aeb3-4a97-814b-42657a2b35c1",
   "metadata": {},
   "source": [
    "# Regridding functions (nearest_neighbor_indexes and nearest_neighbor_interp)\n",
    "\n",
    "    xx_input and yy_input must be 2D arrays\n",
    "    x_output and y_output must be vectors  \n",
    "    field must be either a 2D array with dimensions (y, x) or a 3D array with dimensions (time, y, x) \n",
    "    invalid_values = fill value to replace by 0. Land is therefore considered as open ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1323d0b7-977d-4725-ac96-f50ad6edf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_indexes(x_input, y_input, x_output, y_output):\n",
    "    x_input = np.expand_dims(x_input, axis = 1)\n",
    "    y_input = np.expand_dims(y_input, axis = 1)\n",
    "    x_output = np.expand_dims(x_output, axis = 1)\n",
    "    y_output = np.expand_dims(y_output, axis = 1)\n",
    "    #\n",
    "    coord_input = np.concatenate((x_input, y_input), axis = 1)\n",
    "    coord_output = np.concatenate((x_output, y_output), axis = 1)\n",
    "    #\n",
    "    tree = scipy.spatial.KDTree(coord_input)\n",
    "    dist, idx = tree.query(coord_output)\n",
    "    #\n",
    "    return(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0aa0da65-45ba-49f1-b264-878bd560b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_interp(xx_input, yy_input, x_output, y_output, field, fill_value = None):\n",
    "    xx_input_flat = np.ndarray.flatten(xx_input)\n",
    "    yy_input_flat = np.ndarray.flatten(yy_input)\n",
    "    #\n",
    "    if fill_value is not None:\n",
    "        if field.ndim == 2:\n",
    "            idx_fill_value = np.ndarray.flatten(field) == fill_value\n",
    "        elif field.ndim == 3:\n",
    "            idx_fill_value = np.ndarray.flatten(field[0,:,:]) == fill_value\n",
    "        #\n",
    "        xx_input_flat = xx_input_flat[idx_fill_value == False]\n",
    "        yy_input_flat = yy_input_flat[idx_fill_value == False]\n",
    "    #\n",
    "    xx_output, yy_output = np.meshgrid(x_output, y_output)\n",
    "    xx_output_flat = np.ndarray.flatten(xx_output)\n",
    "    yy_output_flat = np.ndarray.flatten(yy_output)\n",
    "    #\n",
    "    idx = nearest_neighbor_indexes(xx_input_flat, yy_input_flat, xx_output_flat, yy_output_flat)\n",
    "    #\n",
    "    if field.ndim == 2:\n",
    "        field_flat = np.ndarray.flatten(field)\n",
    "        if fill_value is not None:\n",
    "            field_flat = field_flat[idx_fill_value == False]\n",
    "        #\n",
    "        field_interp = field_flat[idx]\n",
    "        field_regrid = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #    \n",
    "    elif field.ndim == 3:\n",
    "        time_dim = len(field[:,0,0])\n",
    "        field_regrid = np.full((time_dim, len(y_output), len(x_output)), np.nan)\n",
    "        #\n",
    "        for t in range(0, time_dim):\n",
    "            field_flat = np.ndarray.flatten(field[t,:,:])\n",
    "            if fill_value is not None:\n",
    "                field_flat = field_flat[idx_fill_value == False]\n",
    "            #\n",
    "            field_interp = field_flat[idx]\n",
    "            field_regrid[t,:,:] = np.reshape(field_interp, (len(y_output), len(x_output)), order = \"C\")\n",
    "    #\n",
    "    return(field_regrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938015e-0d56-4adf-8e97-7947f1fb8387",
   "metadata": {},
   "source": [
    "# Rotate wind function\n",
    "    x_wind, y_wind, lats, lons must be numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be011b1b-d64b-4f82-b359-1ae2e045095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_wind(x_wind, y_wind, lats, lons, proj_str_from, proj_str_to):\n",
    "    if np.shape(x_wind) != np.shape(y_wind):\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} and y_wind {np.shape(y_wind)} arrays must be the same size\")\n",
    "    if len(lats.shape) != 1:\n",
    "        raise ValueError(f\"lats {np.shape(lats)} must be 1D\")\n",
    "    if np.shape(lats) != np.shape(lons):\n",
    "        raise ValueError(f\"lats {np.shape(lats)} and lats {np.shape(lons)} must be the same size\")\n",
    "    if len(np.shape(x_wind)) == 1:\n",
    "        if np.shape(x_wind) != np.shape(lats):\n",
    "            raise ValueError(f\"x_wind {len(x_wind)} and lats {len(lats)} arrays must be the same size\")\n",
    "    elif len(np.shape(x_wind)) == 2:\n",
    "        if x_wind.shape[1] != len(lats):\n",
    "            raise ValueError(f\"Second dimension of x_wind {x_wind.shape[1]} must equal number of lats {len(lats)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"x_wind {np.shape(x_wind)} must be 1D or 2D\")\n",
    "    #\n",
    "    proj_from = pyproj.Proj(proj_str_from)\n",
    "    proj_to = pyproj.Proj(proj_str_to)\n",
    "    transformer = pyproj.transformer.Transformer.from_proj(proj_from, proj_to)\n",
    "    #\n",
    "    orig_speed = np.sqrt(x_wind**2 + y_wind**2)\n",
    "    #\n",
    "    x0, y0 = proj_from(lons, lats)\n",
    "    if proj_from.name != \"longlat\":\n",
    "        x1 = x0 + x_wind\n",
    "        y1 = y0 + y_wind\n",
    "    else:\n",
    "        factor = 3600000.0\n",
    "        x1 = x0 + x_wind / factor / np.cos(lats * 3.14159265 / 180)\n",
    "        y1 = y0 + y_wind / factor\n",
    "    #\n",
    "    X0, Y0 = transformer.transform(x0, y0)\n",
    "    X1, Y1 = transformer.transform(x1, y1)\n",
    "    #\n",
    "    new_x_wind = X1 - X0\n",
    "    new_y_wind = Y1 - Y0\n",
    "    #\n",
    "    if proj_to.name == \"longlat\":\n",
    "        new_x_wind *= np.cos(lats * 3.14159265 / 180)\n",
    "    #\n",
    "    if proj_to.name == \"longlat\" or proj_from.name == \"longlat\":\n",
    "        curr_speed = np.sqrt(new_x_wind**2 + new_y_wind**2)\n",
    "        new_x_wind *= orig_speed / curr_speed\n",
    "        new_y_wind *= orig_speed / curr_speed\n",
    "    #\n",
    "    return(new_x_wind, new_y_wind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a570ca-d994-4605-89df-61c1bd313347",
   "metadata": {},
   "source": [
    "# Read_netCDF functions\n",
    "    filename: filename including the path\n",
    "    variables: list of variables (excluding time, x, y, lat, lon) to extract (list of strings)\n",
    "    paths: dictionary defined in the Constants section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a974d62a-afc1-4ec9-85d7-98ff62a63777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_netCDF(filename, variables, paths = paths):\n",
    "    Dataset = {}\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    Dataset[\"time\"] = nc.variables[\"time\"][:]\n",
    "    #\n",
    "    if paths[\"ECMWF\"] in filename:\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][:]\n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][:]\n",
    "    #\n",
    "    elif paths[\"AMSR2\"] in filename:\n",
    "        xmin = 909\n",
    "        xmax = 1453\n",
    "        ymin = 1075\n",
    "        ymax = 1555\n",
    "        Dataset[\"x\"] = nc.variables[\"xc\"][xmin:xmax] * 1000\n",
    "        Dataset[\"y\"] = nc.variables[\"yc\"][ymin:ymax] * 1000\n",
    "        Dataset[\"lat\"] = nc.variables[\"lat\"][ymin:ymax, xmin:xmax] \n",
    "        Dataset[\"lon\"] = nc.variables[\"lon\"][ymin:ymax, xmin:xmax]\n",
    "    #\n",
    "    for var in variables:\n",
    "        vardim = nc.variables[var].ndim\n",
    "        if vardim == 1:\n",
    "            Dataset[var] = nc.variables[var][:]\n",
    "        elif vardim == 2:\n",
    "            Dataset[var] = nc.variables[var][:,:]\n",
    "        elif vardim == 3:\n",
    "            Dataset[var] = nc.variables[var][:,:,:]\n",
    "            if paths[\"AMSR2\"] in filename:\n",
    "                xmin = 909\n",
    "                xmax = 1453\n",
    "                ymin = 1075\n",
    "                ymax = 1555\n",
    "                Dataset[var] = Dataset[var][:, ymin:ymax, xmin:xmax]\n",
    "                #\n",
    "                filename_date = os.path.basename(filename)[13:21]\n",
    "                if filename_date in Dates_AMSR2_missing_data:\n",
    "                    Dataset[var] = np.full((1, 480, 544), np.nan)\n",
    "                else:\n",
    "                    if (\"ice_conc\" in var) or (\"total_standard_uncertainty\" in var):\n",
    "                        idx_invalid_values = Dataset[var] < 0\n",
    "                        Dataset[var][idx_invalid_values == True] = -32767\n",
    "        else:\n",
    "            print(\"ERROR. Number of dimensions higher than 3.\")\n",
    "    nc.close()\n",
    "    #\n",
    "    return(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0128148-a417-41e7-b798-7a8ccf335b30",
   "metadata": {},
   "source": [
    "# extract_ECMWF_data function\n",
    "\n",
    "    filename: filename (including path) containing ECMWF data \n",
    "    ndays: maximum lead time in days\n",
    "    TOPAZ: TOPAZ dataset (dictionary)   \n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract (variables[\"ECMWF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8aaf49e8-4e05-4212-8f3c-bf99eedddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECMWF_data(filename, ndays, domain, proj = proj, variables = variables[\"ECMWF\"], crs = crs):\n",
    "    ECMWF = read_netCDF(filename, variables)\n",
    "    Data_AMSR2grid = {}\n",
    "    transform_ECMWF_to_AMSR2 = pyproj.Transformer.from_crs(crs[\"ECMWF\"], crs[\"AMSR2\"], always_xy = True)\n",
    "    lons, lats = np.meshgrid(ECMWF[\"lon\"], ECMWF[\"lat\"])\n",
    "    xx_ECMWF_AMSR2proj, yy_ECMWF_AMSR2proj = transform_ECMWF_to_AMSR2.transform(lons, lats)\n",
    "    #\n",
    "    Data_ECMWFgrid = {}\n",
    "    for var in variables:\n",
    "        Data_ECMWFgrid[var] = ECMWF_time_steps_to_daily_time_steps(ECMWF[\"time\"], ECMWF[var], ndays)\n",
    "    #\n",
    "    if (\"U10M\" in Data_ECMWFgrid) and (\"V10M\" in Data_ECMWFgrid):\n",
    "        x_wind = np.full((lead_time_max, len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        y_wind = np.full((lead_time_max, len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        #\n",
    "        for ts in range(0, lead_time_max):\n",
    "            x_wind_rot, y_wind_rot = rotate_wind(np.ndarray.flatten(Data_ECMWFgrid[\"U10M\"][ts,:,:]), \n",
    "                                                 np.ndarray.flatten(Data_ECMWFgrid[\"V10M\"][ts,:,:]),\n",
    "                                                 np.ndarray.flatten(lats), \n",
    "                                                 np.ndarray.flatten(lons), \n",
    "                                                 proj[\"ECMWF\"], \n",
    "                                                 proj[\"AMSR2\"]\n",
    "                                                )\n",
    "            #\n",
    "            x_wind[ts,:,:] = np.reshape(x_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")\n",
    "            y_wind[ts,:,:] = np.reshape(y_wind_rot, (len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), order = \"C\")            \n",
    "            #\n",
    "        Data_ECMWFgrid[\"wind_x\"] = np.copy(x_wind)\n",
    "        Data_ECMWFgrid[\"wind_y\"] = np.copy(y_wind)\n",
    "        Data_ECMWFgrid.pop(\"U10M\")\n",
    "        Data_ECMWFgrid.pop(\"V10M\")\n",
    "    #\n",
    "    Cum_data_ECMWFgrid = {}\n",
    "    for var in Data_ECMWFgrid:\n",
    "        var_cum = np.full((lead_time_max, len(ECMWF[\"lat\"]), len(ECMWF[\"lon\"])), np.nan)\n",
    "        for ts in range(0, lead_time_max):\n",
    "            var_cum[ts,:,:] = np.nanmean(Data_ECMWFgrid[var][0:ts+1,:,:], axis = 0)\n",
    "        #\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"] = np.copy(var_cum)\n",
    "        Cum_data_ECMWFgrid[var + \"_cum\"][np.isnan(var_cum) == True] = -32767\n",
    "    #\n",
    "    for var in Cum_data_ECMWFgrid:\n",
    "        Data_AMSR2grid[var] = nearest_neighbor_interp(xx_ECMWF_AMSR2proj, yy_ECMWF_AMSR2proj, domain[\"x\"], domain[\"y\"], Cum_data_ECMWFgrid[var], fill_value = -32767)\n",
    "    #\n",
    "    return(Data_AMSR2grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cea2e-50cd-41df-b608-26a9319d9b67",
   "metadata": {},
   "source": [
    "# extract_SIC_obs_predictors function\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    domain: domain data (Datasets[\"domain\"]) \n",
    "    trend_period: Number of days to take into account for calculating the trend\n",
    "    proj: dictionary of proj4 strings\n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33244697-1f65-4abe-b6df-e99acbdf4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_SIC_obs_predictors(date_task, domain, proj = proj, crs = crs):\n",
    "    Data_AMSR2 = {}\n",
    "    #\n",
    "    previous_date = datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = 1)\n",
    "    previous_date_str = previous_date.strftime(\"%Y%m%d\")\n",
    "    filename_SIC = paths[\"AMSR2\"] + previous_date_str[0:4] + \"/\" + previous_date_str[4:6] + \"/\" + \"sic_cosi-5km_\" + previous_date_str + \"0000-\" + date_task + \"0000.nc\"  \n",
    "    if os.path.isfile(filename_SIC):\n",
    "        if previous_date_str in Dates_AMSR2_missing_data:\n",
    "            pass\n",
    "        else:\n",
    "            domain_lat = np.expand_dims(domain[\"lat\"], axis = 0)\n",
    "            SIC_data = read_netCDF(filename_SIC, variables = [\"ice_conc\"])\n",
    "            SIC_data[\"ice_conc\"][domain_lat > 89.1] = -32767\n",
    "            xx_SIC, yy_SIC = np.meshgrid(SIC_data[\"x\"], SIC_data[\"y\"])\n",
    "            SIC = nearest_neighbor_interp(xx_SIC, yy_SIC, domain[\"x\"], domain[\"y\"], SIC_data[\"ice_conc\"], fill_value = -32767)\n",
    "            LSM = np.expand_dims(domain[\"LSM\"], axis = 0)\n",
    "            SIC[LSM == 0] = 0            \n",
    "            Data_AMSR2[\"ice_conc\"] = np.squeeze(SIC)\n",
    "    #\n",
    "    return(Data_AMSR2)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4f3bc-d023-4d24-83a4-b04b19d0944d",
   "metadata": {},
   "source": [
    "# extract_targets\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    ndays: Number of days to take into account for calculating the trend\n",
    "    domain: domain data (Datasets[\"domain\"])\n",
    "    proj: dictionary of proj4 strings\n",
    "    variables: list of variables to extract \n",
    "    crs: crs defined in \"Constants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b90fbef-e786-4948-96e4-a05178a53158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(date_task, ndays, domain, paths = paths):\n",
    "    Data_AMSR2 = {}\n",
    "    LSM_extend = np.expand_dims(domain[\"LSM\"], axis = 0)\n",
    "    #\n",
    "    for lt in range(0, ndays):\n",
    "        date_str = (datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt)).strftime(\"%Y%m%d\")\n",
    "        date_day_after_str = (datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt + 1)).strftime(\"%Y%m%d%H%M\")\n",
    "        filename_SIC = paths[\"AMSR2\"] + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"sic_cosi-5km_\" + date_str + \"0000-\" + date_day_after_str + \".nc\"\n",
    "        #\n",
    "        SIC_obs = read_netCDF(filename_SIC, variables = [\"ice_conc\", \"total_standard_uncertainty\"])\n",
    "        if lt == 0:\n",
    "            xx_SIC, yy_SIC = np.meshgrid(SIC_obs[\"x\"], SIC_obs[\"y\"])\n",
    "        #\n",
    "        domain_lat = np.expand_dims(domain[\"lat\"], axis = 0)\n",
    "        SIC_obs[\"ice_conc\"] = SIC_obs[\"ice_conc\"]\n",
    "        SIC_obs[\"total_standard_uncertainty\"] = SIC_obs[\"total_standard_uncertainty\"]\n",
    "        SIC_obs[\"ice_conc\"][domain_lat > 89.1] = -32767\n",
    "        SIC_obs[\"total_standard_uncertainty\"][domain_lat > 89.1] = -32767\n",
    "        SIC = nearest_neighbor_interp(xx_SIC, yy_SIC, domain[\"x\"], domain[\"y\"], SIC_obs[\"ice_conc\"], fill_value = -32767)\n",
    "        total_uncertainty = nearest_neighbor_interp(xx_SIC, yy_SIC, domain[\"x\"], domain[\"y\"], SIC_obs[\"total_standard_uncertainty\"], fill_value = -32767)\n",
    "        #\n",
    "        SIC[LSM_extend == 0] = 0\n",
    "        total_uncertainty[LSM_extend == 0] = 0\n",
    "        #\n",
    "        if lt == 0:\n",
    "            Data_AMSR2[\"SIC\"] = np.copy(SIC)\n",
    "            Data_AMSR2[\"SIC_total_standard_uncertainty\"] = np.copy(total_uncertainty)\n",
    "        else:\n",
    "            Data_AMSR2[\"SIC\"] = np.concatenate((Data_AMSR2[\"SIC\"], SIC), axis = 0)\n",
    "            Data_AMSR2[\"SIC_total_standard_uncertainty\"] = np.concatenate((Data_AMSR2[\"SIC_total_standard_uncertainty\"], total_uncertainty), axis = 0)\n",
    "    #\n",
    "    return(Data_AMSR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160146f-9e53-4626-853f-0a5e56ed3423",
   "metadata": {},
   "source": [
    "# write_netCDF function\n",
    "    date_task: forecast start date (string \"YYYYMMDD\")\n",
    "    Datasets: Dictionary containing all variables that we want to extract\n",
    "    paths: paths defined in the Constants section\n",
    "    trend_period: Number of days to take into account for calculating the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51ef07e8-5bb2-46c7-9134-6227dd3b24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_netCDF(date_task, Datasets, paths):\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    path_output = paths[\"output\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/\"\n",
    "    if os.path.exists(path_output) == False:\n",
    "        os.system(\"mkdir -p \" + path_output)    \n",
    "    output_filename = path_output + \"Dataset_\" + date_task + \".nc\"\n",
    "    if os.path.isfile(output_filename):\n",
    "        os.system(\"rm \" + output_filename)\n",
    "    output_netcdf = netCDF4.Dataset(output_filename, 'w', format = 'NETCDF4')\n",
    "    #\n",
    "    dimensions = [\"time\", \"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        if di == \"time\":\n",
    "            Outputs[di] = output_netcdf.createDimension(di, lead_time_max)\n",
    "        else:\n",
    "            Outputs[di] = output_netcdf.createDimension(di, len(Datasets[\"domain\"][di]))\n",
    "    #\n",
    "    dim_variables = dimensions + [\"lat\", \"lon\"]\n",
    "    for dv in dim_variables:\n",
    "        if dv == \"time\":\n",
    "            time_vect = []\n",
    "            for lt in range(0, lead_time_max):\n",
    "                time_vect.append((datetime.datetime.strptime(date_task, \"%Y%m%d\") + datetime.timedelta(days = lt)).strftime(\"%Y%m%d\"))\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "            Outputs[dv][:] = time_vect\n",
    "            Outputs[dv].standard_name = \"forecast time\"\n",
    "            Outputs[dv].units = \"forecasted_date\"\n",
    "        else: \n",
    "            if Datasets[\"domain\"][dv].ndim == 1:\n",
    "                    Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "                    Outputs[dv][:] = Datasets[\"domain\"][dv]\n",
    "                    if dv == \"x\" or dv == \"y\":\n",
    "                        Outputs[dv].standard_name = \"projection_\" + dv + \"_coordinate\"\n",
    "                        Outputs[dv].units = \"m\"\n",
    "            elif Datasets[\"domain\"][dv].ndim == 2:\n",
    "                Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (\"y\", \"x\"))\n",
    "                Outputs[dv][:,:] = Datasets[\"domain\"][dv]\n",
    "                if dv == \"lat\":\n",
    "                    Outputs[dv].standard_name = \"latitude\"\n",
    "                elif dv == \"lon\":\n",
    "                    Outputs[dv].standard_name = \"longitude\"\n",
    "                Outputs[dv].units = \"degrees\"\n",
    "    #\n",
    "    SIC_variables = [\"ice_conc\", \"fice\", \"SIC\"]\n",
    "    for ds in Datasets:\n",
    "        for var in Datasets[ds]:\n",
    "            if (var in dim_variables) == False:\n",
    "                if var == \"LSM\":\n",
    "                    var_name = \"LSM\"\n",
    "                elif var in SIC_variables:\n",
    "                    var_name = ds + \"_SIC\"\n",
    "                else:\n",
    "                    var_name = ds + \"_\" + var\n",
    "                #\n",
    "                if Datasets[ds][var].ndim == 2:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:] = np.round(Datasets[ds][var], 3)\n",
    "                elif Datasets[ds][var].ndim == 3:\n",
    "                    Outputs[var_name] = output_netcdf.createVariable(var_name, \"d\", (\"time\", \"y\", \"x\"))\n",
    "                    Outputs[var_name][:,:,:] = np.round(Datasets[ds][var], 3)\n",
    "                #\n",
    "                if var in SIC_variables:\n",
    "                    if ds == \"TARGET_AMSR2\":\n",
    "                        Outputs[var_name].standard_name = \"AMSR2 sea ice concentration\"\n",
    "                    elif ds == \"SICobs_AMSR2\":\n",
    "                        Outputs[var_name].standard_name = \"Sea ice concentration from AMSR2 during the day preceding the forecast start date\"\n",
    "                    else:\n",
    "                        Outputs[var_name].standard_name = ds + \" sea ice concentration\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                if var == \"SIC_total_standard_uncertainty\":\n",
    "                    Outputs[var_name].standard_name = \"Total uncertainty (one standard deviation) of concentration of sea ice\"\n",
    "                    Outputs[var_name].units = \"%\"\n",
    "                elif var == \"LSM\":\n",
    "                    Outputs[var_name].standard_name = \"Land sea mask\"\n",
    "                    Outputs[var_name].units = \"1: ocean, 0: land\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_T2M_cum\":\n",
    "                    Outputs[var_name].standard_name = \"ECMWF 2 metre temperature\"\n",
    "                    Outputs[var_name].units = \"K\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_x_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the x direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "                elif ds + \"_\" + var == \"ECMWF_wind_y_cum\":\n",
    "                    Outputs[var_name].standard_name = \"Mean ECMWF wind in the y direction since the forecast start date\"\n",
    "                    Outputs[var_name].units = \"m/s\"\n",
    "    output_netcdf.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5829d-a0ec-4d1e-af56-1d506820b733",
   "metadata": {},
   "source": [
    "# Data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c53f4fb9-064b-491d-bf64-f0a2ecfc12c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_task 20220326\n",
      "(480, 544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:43: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_x_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/4077965583.py:44: RuntimeWarning: invalid value encountered in multiply\n",
      "  new_y_wind *= orig_speed / curr_speed\n",
      "/tmp/ipykernel_1492942/2662577706.py:37: RuntimeWarning: Mean of empty slice\n",
      "  var_cum[ts,:,:] = np.nanmean(Data_ECMWFgrid[var][0:ts+1,:,:], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing time:  142.7898073196411\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "#\n",
    "date_task = task_date(date_min, date_max, task_ID = SGE_TASK_ID)\n",
    "previous_day = (datetime.datetime.strptime(date_task, \"%Y%m%d\") - datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "print(\"date_task\", date_task)\n",
    "if previous_day in Dates_AMSR2_missing_data:\n",
    "    print(\"Missing AMSR2 observations during the day preceding the forecast start date\")\n",
    "else:\n",
    "    filename_ECMWF = paths[\"ECMWF\"] + date_task[0:4] + \"/\" + date_task[4:6] + \"/ECMWF_operational_forecasts_T2m_10mwind_\" + date_task + \"_NH.nc\"\n",
    "    #\n",
    "    Datasets = {}\n",
    "    #\n",
    "    Datasets[\"domain\"] = extract_domain_and_LSM(paths)\n",
    "    #\n",
    "    Datasets[\"ECMWF\"] = extract_ECMWF_data(filename = filename_ECMWF, \n",
    "                                          ndays = lead_time_max, \n",
    "                                          domain = Datasets[\"domain\"], \n",
    "                                          variables = variables[\"ECMWF\"], \n",
    "                                          crs = crs)\n",
    "    #\n",
    "    Datasets[\"SICobs_AMSR2\"] = extract_SIC_obs_predictors(date_task = date_task, \n",
    "                                                          domain = Datasets[\"domain\"],\n",
    "                                                          proj = proj, \n",
    "                                                          crs = crs)    \n",
    "    #\n",
    "    Datasets[\"TARGET_AMSR2\"] = extract_targets(date_task = date_task, \n",
    "                                               ndays = lead_time_max, \n",
    "                                               domain = Datasets[\"domain\"], \n",
    "                                               paths = paths\n",
    "                                               )\n",
    "    #\n",
    "    write_netCDF(date_task = date_task, \n",
    "                 Datasets = Datasets, \n",
    "                 paths = paths, \n",
    "                 )    \n",
    "    #\n",
    "    tf = time.time() - t0\n",
    "    print(\"Computing time: \", tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e5ff6-d301-4b33-99b8-3ca1c43cc616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dc384-dd23-4c49-aebe-4ec93db97395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4a33c-8496-4a7a-b1e2-c3ad0cd25e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f417f1d-0bd6-40ae-8209-b97bc29788d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
